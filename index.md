## EN.580.709 - Fall 2019 - Johns Hopkins University
### Jeremias Sulam
M 3:00PM - 5:00PM
Credits:  2


### Course Description 
Sparse and redundant representations constitutes a fascinating area of research in signal and image processing. This is a relatively young field that has been taking form for the last 15 years or so, with contributions from harmonic analysis, numerical algorithms and machine learning, and has been vastly applied to myriad of problems in computer vision and other domains. This course will focus on sparsity as a model for general data, generalizing many different other constructions or priors. This idea - that signals can be represented with just *a few* coefficients - leads to a long series of beautiful (and surprisingly, solvable) theoretical and numerical problems, and many applications that can benefit directly from the new developed theory. In this course we will survey this field, starting with the theoretical foundations, and systematically covering the knowledge that has been gathered in the past years. This course will touch on theory, numerical algorithms, and applications in image processing and machine learning. Recommended course background: Linear Algebra, Signals and Systems, Numerical Analysis.


### Content and Syllabus
This could will cover the fundamental ideas behind sparse signal and image modeling, as well as key ideas of sparse priors in machine learning, and their applications. The (tentative) roadmap is the following:

1. Intro to course and underdetermined linear systems of equations
2. Uniqueness and Stability
3. Greedy Algorithms and their analysis
4. Basis Pursuit - success guarantees and stability. Analysis and convergence of ISTA
5. Intro to statistical learning, sparse linear and logistic regression, variable selection
6. Lasso generalizations, matrix and spectral sparsity and applications
7. Dictionary Learning - MOD, K-SVD, Online Dictionarly Learning, Double Sparsity
8. Computer Vision Applications: image denoising, inpaing, compression, MCA
9. Convolutional sparsity and dictionary learning, and their application
10. MultiLayer sparsity based deep learning
11. Project work
12. Project work
13. Project Presentation
14. Project Presentation

### Course Requirements and Grading

The course will be organized in weekly lectures, and students are expected to actively participate in them. Grading will be assigned according to:

a) **Final exam** (30%), intended to serve as a fast check for contents presented in class.

b) **Course Project** (70%), to be carried out preferably in pairs, based on recently published papers. A list of candidate papers to choose from is given <a href="https://github.com/jsulam-jhu/EN.580.709/blob/master/project_papers.md">here</a>, (others can be accommodated upon request). Evaluation of the project will be based on a final report summarizing some of these papers, their contributions, and your own findings (open questions, simulation results, extensions, etc), as well as a short presentation to the class.


### References

* Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing, by M. Elad
* Statistical Learning with Sparsity: The Lasso and Generalizations by T. Hastie et al.
* Recent papers

