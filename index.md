## EN.580.709 - Fall 2019 - Johns Hopkins University
### Jeremias Sulam
M 3:00PM - 5:00PM
Credits:  2


### Course Description 
Sparse and redundant representations constitutes a fascinating area of research in signal and image processing. This is a relatively young field that has been taking form for the last 15 years or so, with contributions from harmonic analysis, numerical algorithms and machine learning, and has been vastly applied to myriad of problems in computer vision and other domains. This course will focus on sparsity as a model for general data, generalizing many different other constructions or priors. This idea - that signals can be represented with just *a few* coefficients - leads to a long series of beautiful (and surprisingly, solvable) theoretical and numerical problems, and many applications that can benefit directly from the new developed theory. In this course we will survey this field, starting with the theoretical foundations, and systematically covering the knowledge that has been gathered in the past years. This course will touch on theory, numerical algorithms, and applications in image processing and machine learning. Recommended course background: Linear Algebra, Signals and Systems, Numerical Analysis.


### Content and Syllabus
This could will cover the fundamental ideas behind sparse signal and image modeling, as well as key ideas of sparse priors in machine learning, and their applications. The (tentative) roadmap is the following:

1. Intro to course and underdetermined linear systems of equations
2. Uniqueness and Stability
3. Greedy Algorithms and their analysis
4. Basis Pursuit - success guarantees and stability. Analysis of ISTA, Convergence.
5. Intro to statistical learning, sparse linear and logistic regression, variable selection
6. Lasso generalizations, matrix and spectral sparsity and applications
7. Dictionary Learning - MOD, K-SVD, Online Dictionarly Learning, Double Sparsity. 
8. Computer Vision Applications: image denoising, inpaing, compression, MCA.
9. Convolutional sparsity and dictionary learning, and their application
10. MultiLayer sparsity based deep learning
11. Project work
12. Project work
13. Project Presentation
14. Project Presentation

### Course Requirements and Grading

The course will be organized in weekly lectures, and students are expected to actively participate in them. Grading will be assigned according to:
a) Final exam (30%)
b) Project (70%)

The final written exam is intended to be relatively short and serve as a fast check for the concepts covered in the lectures. 

### References

